---
description: Nous avons vu prÃ©cÃ©demment qu'il Ã©tait possible de compiler puis de packager une application grÃ¢ce Ã  Gitlab-CI, dans ce TP nous allons voir comment dÃ©ployer cette image (**prÃ©sente dans un registry privÃ©e**) dans un cluster Kubernetes
---

# DÃ©ployer une Image Docker dans Kubernetes

Nous avons vu prÃ©cÃ©demment qu'il Ã©tait possible de compiler puis de packager une application grÃ¢ce Ã  Gitlab-CI, dans ce TP nous allons voir comment dÃ©ployer cette image (**prÃ©sente dans un registry privÃ©**) dans un cluster Kubernetes

::: details Sommaire
[[toc]]
:::

## Introduction

Dans ce TP nous allons voir comment :

- Configurer un cluster Kubernetes simple avec `k3d`.
- Comment autoriser le cluster Kubernetes Ã  Â« pull Â» l'image depuis le repo privÃ© de Gitlab.
- Comment lancer & autoriser les connexions sur un port accessible depuis le rÃ©seau.

::: tip Contenu non exhaustif
Kubernetes est un sujet trÃ¨s large qui est trÃ¨s large. Dans ce TP nous poserons uniquement les bases, celle-ci vous servirons Ã  dÃ©couvrir le fonctionnement de Kubernetes, mais Ã©galement comme l'utiliser dans un usage Â« avancÃ©s Â» c'est-Ã -dire sans forcÃ©ment utiliser une image sur le hub public de Docker.
:::

## Le Cluster

Contrairement Ã  un simple Docker, compose Kubernetes reposes sur un principe de Cluster. Le cluster n'est pas Â« une simple image Â». Il sâ€™agit d'un outil d'orchestration qui regroupera Ã  la fois :

- Les images (container).
- Les paramÃ©trages des volumes.
- Le nombre d'instances dÃ©ployÃ©.
- Le rÃ©seau
- En passant par l'exposition des services.

L'idÃ©e ici est donc de gÃ©rer l'ensemble de votre Â« stack Â» et pas seulement la combinaison d'images dans un Docker-Compose. L'ensemble de votre configuration sera configurÃ©e en YAML dans _un_ ou **plusieurs** fichiers.

Nous allons avoir plusieurs possibilitÃ©s pour crÃ©er notre cluster, il existe plusieurs Â« logiciels Â» permettant de crÃ©er des Clusters Kubernetes :

- k8s
- minikube
- k3s
- k3d (en rÃ©alitÃ© c'est k3s in Docker)
- kind
- â€¦

Beaucoup de possibilitÃ© pour rÃ©pondre Ã  des cas d'usage diffÃ©rents, le plus simple dans notre cas c'est Â« k3d Â» ; pourquoi ? Car il permet de dÃ©ployer un cluster Kubernetes dans un environnement conteneurisÃ© type Docker.

::: warning Du YAML ?
Ouiâ€¦ Beaucoup de YAML ! Mais vous allez voirâ€¦ Une fois une bonne base en place c'est Â« plutÃ´t simple Â».

![YAML](./res/yaml.jpeg)

:::

::: danger GÃ©rer son clusterâ€¦ kubectl ? helm ?
Le monde Kubernetes est rempli de plusieurs outils, les diffÃ©rents outils ont Ã©tÃ© crÃ©Ã© Ã  diffÃ©rentes Ã©poques et rÃ©pondent Ã  des besoins diffÃ©rents, et surtout des tailles de projet diffÃ©rentes.

Pour cette introduction, je vais rester sur l'outil de base Ã  savoir `kubectl`, il sera amplement suffisant et vous les verrez il nous permettra mÃªme de dÃ©ployer sans trop de difficultÃ©s en automatique dans un flow de CI/CD.
:::

### k3d

Nous allons donc installer [k3d](https://k3d.io/), l'installation va Ãªtre relativement simple, il s'agit ici juste d'un petit outil qui nous permettra de crÃ©er / initialiser, mais Ã©galement de manager nos diffÃ©rents clusters.

Pour l'installation, je vous laisse vous reporter [Ã  la documentation officielle](https://github.com/rancher/k3d#get) en effet, en fonction de votre OS l'installation sera Ã©videmment diffÃ©rente.

Une fois installÃ© sur votre machine / serveur, vous devriez pouvoir jouer dans votre terminal la commande :

```sh
$ k3d --version
k3d version v4.4.3
k3s version v1.20.6-k3s1 (default)
```

Dans mon cas, j'ai donc la version `4.4.3` de k3d.

#### Initialiser un Cluster

Pour l'instant nous n'avons pas initialisÃ© de cluster, notre machine / serveur est toujours identique Ã  avant l'installation. La crÃ©ation de clusters va se rÃ©aliser via la commande `k3d` rÃ©cemment installÃ©e. Pour faire simple, si vous souhaitez crÃ©er un cluster, il vous suffira de :

```sh
k3d cluster create --api-port IP_DE_VOTRE_SERVEUR:20135 -p "8080:80@loadbalancer" --volume $(pwd)/volume/:/data/ -s 1 -a 2 monCluster
```

**Avant de jouer la commande comme un sauvage**, dÃ©taillons un peu ce que vous allez lancer sur votre machine :

![Infra](./res/infra.jpg)

| Options                                 | Usage                                                                                       |
| --------------------------------------- | ------------------------------------------------------------------------------------------- |
| `cluster`                               | Indique que nous souhaitons gÃ©rer la partie cluster                                         |
| `create`                                | Indique que nous souhaitons crÃ©er un nouveau cluster                                        |
| `--api-port IP_DE_VOTRE_SERVEUR::20125` | Port d'Ã©coute de la partie API **de management** du cluster                                 |
| `-p "8080:80@loadbalancer"`             | Expose le port `8080` sur votre machine, il permettra d'accÃ©der Ã  votre Â« service dÃ©ployÃ© Â» |
| `--volume ./volume/:/data/`             | Fournis un espace de stockage persistant Ã  votre cluster                                    |
| `-s 1`                                  | Indique que vous souhaitez 1 serveur                                                        |
| `-a 2`                                  | Indique que vous souhaitez 2 agents                                                         |
| `monCluster`                            | Le nom du cluster que vous souhaitez crÃ©er                                                  |

Vous pouvez maintenant lancer la commande. Dans quelques minutes / secondes en fonctions de votre machine, vous aurez un cluster Kubernetes disponible.

::: tip C'est dans Â« du docker Â»

Vous pouvez vÃ©rifier que nous sommes bien dans diffÃ©rents Container Docker via un simple `docker ps`.

![Docker PS](./res/docker_ps.png)

:::

Votre cluster Kubernetes est maintenant **opÃ©rationnel**, vous pouvez dÃ©ployer dessus ce que vous souhaitez. Si vous avez initialisÃ© le cluster sur votre machine, celui-ci est normalement disponible immÃ©diatement.

Cependant, je vous invite Ã  rÃ©aliser un cluster Â« distant Â» ; mÃªme si fondamentalement Ã§a ne change pas grand-chose au fonctionnement, vous devrez vous poser des questions toutes bÃªtes comme par exemple :

Â« Mais comment j'accÃ¨de Ã  distance Ã  mon Cluster ? Comment est gÃ©rÃ©e la sÃ©curitÃ© de mon cluster ? Â»

### AccÃ¨s du cluster Ã  distance

Nous allons donc voir comment Â« rÃ©cupÃ©rer Â» la configuration du cluster `monCluster` que nous avons initialisÃ© prÃ©cÃ©demment.

::: warning Pourquoi est-ce important Ã  mon avis ?

Dans le monde rÃ©el, il est peu probable que votre cluster Kubernetes soit en local sur votre serveur, l'intÃ©rÃªt est trÃ¨s limitÃ© et l'ajout de Kubernetes serait trÃ¨s subjectifâ€¦

MÃªme si ce TP utilise une implÃ©mentation Â« simple Â» d'un cluster Kubernetes, il peut complÃ¨tement Ãªtre utilisÃ© dans le cadre d'un environnement de dÃ©veloppement. Et dans ce cadre, il est Ã©vident quâ€™accÃ©der Ã  son cluster Ã  distance est primordial.

:::

k3d (ou k3s in Docker) utilise la mÃªme logique d'authentification que les autres solution Kubernetes du marchÃ©. Ã‡a repose comme souvent avec Kubernetes sur â€¦ un fichier YAML, celui-ci va contenir l'ensemble de la configuration et des clÃ©s d'accÃ¨s de votre cluster.

Il est donc **important que celui-ci reste privÃ©**, vous ne devez jamais le partager, le comitÃ© dans un dÃ©pot public, voire le diffuser en ligne. Car Ã©videmment celui-ci donne les pleins pouvoirs pour modifier / dÃ©ployer / interagir avec votre Cluster.

Pour forcer `k3d` Ã  Ã©crire la configuration de votre Cluster, il faut saisir la commande:

```sh
$ k3d kubeconfig write monCluster
/home/vbrosseau/.k3d/kubeconfig-monCluster.yaml
```

Ã€ cette Ã©tape, vous venez d'Ã©crire la configuration de votre Cluster dans votre serveur. Cette configuration va nous permettre par la suite de piloter le cluster via **votre ordinateur**.

Le fichier est sauvegardÃ© dans le dossier `.k3d` dans la home de votre utilisateur. Vous pouvez la rÃ©cupÃ©rer via un `scp` (ou autre solution de transfert).

```sh
scp vbrosseau@monServer.dev:.k3d/kubeconfig-monCluster.yaml .
```

Et voilÃ  vous avez maintenant l'ensemble des `secrets` nÃ©cessaires pour que votre serveur vous reconnaisse ğŸ‘‹. Pour les fournir Ã  `kubectl` il suffit de :

```sh
export KUBECONFIG=kubeconfig-monCluster.yaml
kubectl cluster-info
```

Si vous avez bien suiviâ€¦ vous devriez avoir les informations de votre cluster.

::: danger STOP !
La curiositÃ© n'est pas un vilain dÃ©faut ! Regardons ensemble le contenu du fichier que vous venez de rÃ©cupÃ©rer !
:::

Votre Cluster est maintenant pleinement fonctionnel. Il est pour l'instant vide, mais vous allez voir, nous allons trÃ¨s prochainement dÃ©ployer votre application.

### kubectl ?

`kubectl` est l'outil officiel permettant de gÃ©rer votre cluster Kubernetes. [Pour l'installation je vous laisse suivre la documentation officielle](https://kubernetes.io/docs/tasks/tools/) :

- [Linux](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/) (ou via votre gestionnaire de paquet)
- [Windows](https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/)
- [MacOS](https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/) (ou via brew)

### Les solutions payantes

Bon, mÃªme si ce nâ€™est pas le sujet de ce TP, il faut avoir en tÃªte qu'il existe Ã©normÃ©ment de solutions pour monter un Cluster Kubernetes. Ici nous avons crÃ©Ã© un cluster avec `k3s` (via `k3d`), aucun problÃ¨me votre cluster va fonctionner comme attendu. **Cependant** cette solution est valide pour expÃ©rimenter Kubernetes ou pour dÃ©ployer une application perso.

Si vous souhaitez utiliser Kubernetes **en prod** ou de maniÃ¨re professionnelle, je vous conseille vivement de passer par une solution manager du type :

- [Kubernetes sur AWS](https://aws.amazon.com/fr/kubernetes/)
- [Google Engine Kubernetes](https://cloud.google.com/kubernetes-engine)
- [OVH Kubernetes](https://www.ovhcloud.com/en/public-cloud/kubernetes/)
- [Kubernetes Kapsule de Scaleway](https://www.scaleway.com/en/kubernetes-kapsule/)

L'usage de ce type de solution ne change rien Ã  ce que nous avons vu / allons voir. Leur avantage c'est que vous n'avez rien Ã  gÃ©rer, la solution est **managÃ©e** par le fournisseur de service.

## CrÃ©er l'image Docker via Gitlab-CI

Avant d'aller plus loinâ€¦ Il nous faut quelque chose Ã  dÃ©ployer. Dans Kubernetes nous pouvons dÃ©ployer diffÃ©rents types d'application :

- Des outils sans Â« interface Â» (HTML, ou API).
- Des API (exposÃ©e en HTTP).
- Un site Internet.
- Une base de donnÃ©es
- Une combinaison de site Internet + API + BDD

Bref, ce que vous souhaitez donc !

### Le projet

Pour dÃ©buter nous allons dÃ©ployer un projet simple, je vous propose de dÃ©ployer un projet similaire Ã  la documentation que vous Ãªtes en train de lire. La solution que j'utilise s'appelle [VuePress](https://vuepress.vuejs.org/), cette solution permet de crÃ©er rapidement un site Ã  partir de fichier `Markdown` nous sommes donc en plein dans la JamStack.

**-> Je vous laisse regarder un peu la documentation de VuePress avant de continuer**

Pour crÃ©er un projet VuePress, il suffit d'entrer la commande :

```sh
npx create-vuepress-site vuePressInKube
```

::: tip Au passage
Je vous avais parlÃ© de `pnpm` prÃ©cÃ©demmentâ€¦ Ici si vous souhaitez l'utiliser Ã©galement vous pouvez faire `pnpx` Ã  la place Ã  `npx`.
:::

Avant d'aller plus loin, je vous laisse :

- Tester que Ã§a fonctionne en local avec un `npm install` puis un `npm run dev` (attention Ã  Ãªtre dans le dossier `docs`).
- CrÃ©er votre projet sur Gitlab et crÃ©er une premiÃ¨re version du dossier `docs`.

Pour le premier point si tout va bien vous devriez avoir :

![VuePress](./res/init.png)

### Le Docker File

Si tout est bon, continuons, nous allons maintenant crÃ©er une image Docker de votre site. C'est celle-ci que nous dÃ©ploierons dans notre container.

Ici il s'agit d'un site Internet, donc comme [dans le TP sur le packaging avec Docker](/tp/ci/packager-docker.md) nous utiliserons une image Â« simpliste Â» Ã  base de nginx.

Cette image aura pour but de prendre le rÃ©sultat du Build de VuePress pour l'hÃ©berger de maniÃ¨re statique. La premiÃ¨re version est donc de crÃ©er une premiÃ¨re version via :

```sh
pnpm run build
```

Votre site static est gÃ©nÃ©rÃ© dans le dossier `src/.vuepress/dist` c'est ce dossier que nous allons mettre dans notre image Docker.

Je vous laisse crÃ©er le fichier `Dockerfile` dans le dossier `docs` avec le contenu suivant

```yaml
FROM nginx:stable-alpine
COPY src/.vuepress/dist /usr/share/nginx/html
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

::: danger STOP !

Avant de pusher votre code, tester sur votre ordinateur le bon fonctionnement !

```sh
docker build -t vuepress:test .
docker run -it -p 9090:80 --rm --name vuepresstest vuepress:test
```

Â« Votre site Â» doit-Ãªtre accessible sur [le port 9090](http://localhost:9090)

:::

### La CI

L'idÃ©e dans ce TP est de faire gÃ©nÃ©rer l'image Docker automatiquement par Gitlab-CI afin de la rÃ©cupÃ©rer de maniÃ¨re privÃ©e dans notre cluster Kubernetes. Nous allons donc configurer la CI de Gitlab pour gÃ©nÃ©rer une nouvelle image Docker Ã  chaque fois qu'un commit arrive dans la `master`.

Cette image Docker sera taguÃ©e avec l'identifiant du commit en question. Cette Ã©tape est **identique** Ã  ce que nous avons dÃ©jÃ  fait prÃ©cÃ©demment dans le TP [Packager avec Docker](/tp/ci/packager-docker.md#packager).

Je vous laisse regarder comment nous avions fait, afin de reprendre la mÃªme logique dans ce projet. En deux mots l'opÃ©ration sera la suivante :

- CrÃ©er un fichier `.gitlab-ci.yml` Ã  la racine de votre projet.
- Ajouter la configuration qui permettra de `builder le projet` puis de `builder l'image docker`.
- Commiter et pusher votre projet pour que Gitlab-CI lance la compilation.

**Attention**, dans le cas prÃ©sent, les Ã©tapes de compilation Â« JS Â» seront plus simples que dans l'exemple du TP dont vous vous inspirez, un simple `npm run build` sera certainement suffisant ;).

::: warning Tester c'est douter ?
Avant d'envoyer l'image dans notre cluster Kubernetes, je vous propose de tester que celle-ci fonctionne correctement. AprÃ¨s le build, tester de la rÃ©cupÃ©rer pour la lancer sur votre Docker local.
:::

::: details En manque d'inspiration ?

Avez-vous vraiment cherchez ? Si ouiâ€¦ VoilÃ  un exemple de `.gitlab-ci.yml` qui fonctionne :

```yml
stages:
  - build
  - release

build:
  image: node:latest
  stage: build
  script:
    - npm install
    - npm run build
  artifacts:
    paths:
      - src/.vuepress/dist
  only:
    - master

release:
  image: docker:19.03.12
  stage: release
  dependencies:
    - build
  services:
    - docker:19.03.12-dind
  variables:
    IMAGE_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA
  script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker build -t $IMAGE_TAG .
    - docker push $IMAGE_TAG
  only:
    - master
```

:::

### L'image Docker

Si tout s'est bien passÃ©, vous avez maintenant une premiÃ¨re version de votre application dans votre Registry PrivÃ©e, celle-ci contient une version de votre application.

Le registry Ã©tant privÃ©, nous allons devoir autoriser le cluster Kubernetes Ã  communiquer avec celui-ci. Rien de bien compliquÃ© rassurez-vous.

![CI Success](./res/ci-success.png)
![Registry Gitlab](./res/registry-image.png)

## DÃ©ployer l'image sur le cluster Kubernetes

Nous attaquons maintenant la partie qui vous intÃ©resse, j'imagine :smile:, comment dÃ©ployer cette image dans notre cluster Kubernetes.

### Introduction

Comme je disais en introduction nous allons devoir Ã©crire quelques fichiers YAML. Ces fichiers ont chacun une fonction :

- `deployment.yaml` va contenir l'ensemble des paramÃ¨tres liÃ©s Ã  votre dÃ©ploiement (images Ã  dÃ©ployer, nombre de replicas, nom de votre projet, la rÃ©fÃ©rence Ã  vos secrets de pull Docker).
- `services.yaml` va indiquer le ou les ports disponibles Ã  l'intÃ©rieur de votre/vos image(s).
- `ingress.yaml` va indiquer comment le port ou les ports doivent-Ãªtre exposÃ© Ã  vos clients (path, ou sur un domaine en particuliÃ©)

Ces fichiers sont Â« presque Â» toujours identiques entre chaque dÃ©ploiement, c'est pour Ã§a que des solutions comme `helm` existent. Pour simplifier, dans notre cas, je vais vous donner les fichiers.

Cependant, si vous souhaitez vraiment maÃ®triser ce que nous sommes en train de voir, je vous invite vivement Ã  regarder ce que vous indiquez dans les fichiers.

::: tip Les paramÃ¨tres importants ?
MÃªme si tous les paramÃ¨tres sont importants deux doivent attirer votre attention :

- le `name`, prÃ©sent dans l'ensemble des fichiers, c'est le nom de votre projet Ã  dÃ©ployer.
- le `image`, c'est le lien vers votre image Docker Ã  dÃ©ployer (privÃ©e ou publique). Si l'image est privÃ©e, il faudra indiquer un `imagePullSecrets`.

![Fichier deployment.yml](./res/deployment_yaml_file.png)
:::

### Le secret

Nous l'avons vu tout Ã  l'heure, pour communiquer avec votre Cluster il faut Ãªtre authentifiÃ©. Attention si vous avez fermÃ© le terminal depuis il faut exporter Ã  nouveau la variable `$KUBECONFIG`.

```sh
export KUBECONFIG=~/emplacement/vers/le/secret/kubeconfig-monCluster.yaml
```

### L'authentification avec le Registry Gitlab

MÃªme si il est complÃ¨tement possible d'utiliser le Docker Hub j'ai fait le choix de vous montrer directement comment utiliser une image sur un `Registry privÃ©`. Pourquoi ? Ã€ mon sens, c'est trÃ¨s certainement la premiÃ¨re problÃ©matique que vous rencontrerez. En effet dans le cadre du dÃ©ploiement continu Ã  part travailler sur un projet Â« Open Source Â» publique il y a fort Ã  parier que votre entreprise ne souhaite pas vraiment avoir son code source disponible publiquement en ligneâ€¦

**C'est pour Ã§a qu'il est important** de maÃ®triser cet aspect. Kubernetes est complÃ¨tement capable d'utiliser le Registry de Gitlab, il faut juste lui donner Â« vos identifiants Â». Ã‰videmment vous n'aller pas donner votre login et votre mot de passe.

| <iframe src="https://giphy.com/embed/gIfdqZA4ECvMVrRpSv" width="480" height="360" frameBorder="0" class="giphy-embed" allowFullScreen></iframe> |
| :---------------------------------------------------------------------------------------------------------------------------------------------: |
|                            On est bien d'accord, vous ne DONNEZ JAMAIS VOTRE LOGIN ET VOTRE MOT DE PASSE. **JAMAIS**                            |

Cette fois-ci pas de YAML, mais **une simple commande** :

```sh
kubectl create secret docker-registry gitlab-registry --docker-server="https://registry.gitlab.com" --docker-username="VOTRE_UTILISATEUR_GITLAB" --docker-password="TOKEN_OBTENU_PAR_GITLAB" --docker-email="VOTRE_EMAIL_GITLAB" -o yaml --dry-run=client | kubectl apply -f -
```

Pour gÃ©nÃ©rer le Token, il suffit de passer par les paramÃ¨tres de votre profil :

![Token crÃ©ation](./res/token.png)

::: warning Be curious !
Inspectez, regardez, questionnez-moi, l'important est de comprendre ce que vous Ãªtes en train de faire. Dans le cas prÃ©sent tenter de jouer la commande sans la fin (`| kubectl apply -f -`), vous allez voir le contenu de la configuration envoyÃ© Ã  votre cluster Kubernetes.

Et ouiâ€¦ C'est encore du YAML :cry:
:::

### La configuration

Cette partie, je vous la donne Â« pour dÃ©buter Â». Je vous laisse cependant ajuster les diffÃ©rents paramÃ¨tres dans les diffÃ©rents fichiers.

| ![Fichier deployment.yml](./res/deployment_yaml_file.png) |
| :-------------------------------------------------------: |
|              Exemple dans le deployment.yml               |

::: tip On commit, ou on ne commit pas ?
Gros dÃ©batâ€¦ Dans un projet privÃ© pas de problÃ¨me, cette configuration peut accompagner le projetâ€¦ dans le cas d'un projet Â« public Â» attention Ã  ne pas commiter un YAML qui ferait rÃ©fÃ©rence Ã  des informations privÃ©es / non destinÃ©e aux publiques (IP, port, â€¦)
:::

::: danger Par contre
Ce qui est certain par contre, c'est que nous ne commiterons **jamais** le fichier `kubeconfig-monCluster.yaml` qui contient les secrets de votre cluster.
:::

#### Le deployment.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vuepress-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vuepress-test
  template:
    metadata:
      labels:
        app: vuepress-test
    spec:
      containers:
        - name: vuepress-test
          image: registry.gitlab.com/vbrosseau/vuepress-kubernetes-deploy:bb2d2d0b
      imagePullSecrets:
        - name: gitlab-registry
```

::: danger image
N'oubliez pas de changer le lien de l'image vers **votre** image dans le registry gitlab.
:::

#### Le services.yml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: vuepress-test
spec:
  selector:
    app: vuepress-test
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
```

#### Le ingress.yml

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: vuepress-test
  annotations:
    ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: vuepress-test
                port:
                  number: 80
```

::: tip Ã€ quoi Ã§a sert ?
Le fichier Ingress dÃ©finit comment votre pod va Ãªtre accessible. Nous avons donc ici la correspondance entre le chemin (path, ou domain) et le port du service dÃ©fini plus haut.
:::

### DÃ©ployer votre application

Pour le dÃ©ploiement, c'est plutÃ´t simple il suffit de communiquer avec votre serveur avec `kubectl` :

```sh
kubectl apply -f deployment.yaml
kubectl apply -f services.yaml
kubectl apply -f ingress.yaml
```

Le dÃ©ploiement va prendre quelques minutes, vous pouvez le suivre avec les commandes suivante :

- Obtention de l'Ã©tat du pod `kubectl describe pod vuepress-test`
- VÃ©rification de dÃ©ploiement `kubectl get deployments`
- VÃ©rification des pod(s) qui tourn(ent) `kubectl get pods`

Pour la configuration des services et de l'ingress :

- VÃ©rification de l'application : `kubectl get services`
- VÃ©rification de leur application : `kubectl get ingress`

### Tester

Votre application est maintenant disponible, si vous vous souvenez quand nous avons crÃ©Ã© le cluster nous avons indiquÃ© un port pour le load balancer. Si vous n'avez rien changÃ©, c'est le `8080`. Rendez-vous Ã  `IP.DE.VOTRE.SERVEUR:8080` pour voir votre dÃ©ploiement.

Je vous laisse regarder Ã  nouveau :

- Le `docker ps`.
- Le `kubectl get pods`.

## DÃ©ployer une mise Ã  jour

DÃ©ployer une nouvelle version va Ãªtre beaucoup plus simple **(c'est la force de toute cette construction)**. Je vous laisse procÃ©der :

- Modifier le code (page, thÃ¨me, etc).
- CrÃ©er un commit et pusher la modification.
- Attendez que la nouvelle version de votre application soit dans le registry gitlab.

::: tip Ã€ votre avis
Selon vous, comment allons-nous dÃ©ployer une nouvelle version de votre application dans le cluster ?

- Quel fichier allons-nous modifier ?
- Quelle commande allons-nous faire pour dÃ©ployer une nouvelle version ?

:::

### Modifier le deployment.yml

DÃ©ployer une nouvelle version de notre application va Ãªtre trÃ¨s trÃ¨s simple. Maintenant que nous avons une nouvelle version de notre application dans le registry, il suffit de :

- Modifier l'image dans le fichier `deployment.yml`
  - Dans mon cas`image: registry.gitlab.com/vbrosseau/vuepress-kubernetes-deploy:LE_NOUVEAU_HASH`
- Appliquer la nouvelle configuration sur le Cluster `kubectl apply -f deployment.yaml`

Et c'est tout ! Patientez une ou deux minutes, votre modification est en ligne !

## La suite ?

Je pense que vous avez compris la suite ? C'est simple de redÃ©ployer, tellement simple que l'automatiser va Ãªtre Ã©galement trÃ¨s simple!

La suite de cette introduction Ã§a va Ãªtre le dÃ©ploiement automatisÃ© en cas de mise Ã  jour du projet. [La suite c'est par ici =>](./cd-avec-kubernetes.md)

PS: Ne vous inquiÃ©tez pasâ€¦ Vous avez clairement fait le plus difficile.

## La gestion des domaines

Nous avons couvert un usage de base dans les premiÃ¨res Ã©tapesâ€¦ C'est bien ! Mais nous pouvons aller plus loin pour utiliser toute la puissance de Kubernetes.

Avec Kubernetes, comme pour un serveur classique, nous avons plusieurs possibilitÃ©s pour exposer nos services :

- Sur un port / domaine.
- Via un chemin personnalisÃ© (ex: `/api`).
- Via un sous-domaine (ex: `api.domain.tld`).

### Gestion multi paths

Exemple, si vous souhaitez exposer **dans le mÃªme cluster** deux applications diffÃ©rentes, il est possible de spÃ©cifier un path pour chaque application. Voici un exemple des deux dÃ©finitions.

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: vitejs-sample
  annotations:
    traefik.ingress.kubernetes.io/rewrite-target: /
    ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: vitejs-sample
                port:
                  number: 80
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api-sample
  annotations:
    traefik.ingress.kubernetes.io/rewrite-target: /
    ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
    - http:
        paths:
          - path: /api
            pathType: Prefix
            backend:
              service:
                name: api-sample
                port:
                  number: 80
```

Notre cluster va donc exposer :

- `/` vers le service `vitejs-sample`
- `/api` vers le service `api-sample`

::: danger Ã‰videmment il s'agit d'un exemple
J'ai rÃ©sumÃ© ici la partie ingress seulement et avec des noms fictifs. Les Ã©lÃ©ments importants dans l'extrait prÃ©cÃ©dent sont :

- `traefik.ingress.kubernetes.io/rewrite-target: /`, ce paramÃ¨tre permet de masquer au pod (container), que celui-ci est exposÃ© sur un chemin diffÃ©rent que celui prÃ©vu.
- `- path: /api` spÃ©cifie le path Â« personnalisÃ© Â» associÃ© au service

:::

C'est Ã©videmment un exemple Ã  adapter. Je vous conseille autant que possible de passer par un multidomaine si possible.

### Gestion multidomaine

Dans l'exemple prÃ©cÃ©dent, nous avons dÃ©ployÃ© une seule application dans notre cluster ; mÃªme si d'un point de vue segmentation c'est plus propre, dans les faits il est complÃ¨tement possible de dÃ©ployer plusieurs Â« applications Â» diffÃ©rentes dans un mÃªme cluster. C'est d'ailleurs le cas si votre application possÃ¨de plusieurs services (API, Web et Base de donnÃ©es par exemple).

L'idÃ©e ici, c'est d'indiquer comment le trafic doit Ãªtre routÃ© vers les containers. Cette configuration est spÃ©cifiÃ©e dans les fichiers `ingress.yaml` de vos diffÃ©rentes applications.

Par exemple, si je souhaite :

![Cluster multi](./res/cluster-multi.png)

Je vais devoir indiquer dans mon YAML de configuration :

#### `ingress.yaml` vuepress-test

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: vuepress-test
  annotations:
    ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
    - host: press.domain.tld
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: vuepress-test
                port:
                  number: 80
```

Dans ce cas, le service `vuepress-test` sera accessible sur le domaine `press.domain.tld` et uniquement sur ce domaine.

#### `ingress.yaml` vitejs-sample

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: vitejs-sample
  annotations:
    ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: vitejs-sample
                port:
                  number: 80
```

Ce qui donnera aprÃ¨s application de la configuration :

![Ingress multi result](./res/ingress-multi.png)

::: tip Vous souhaitez tester ?
Cet usage est un peu plus compliquÃ© Ã  mettre en place, mais c'est clairement possible. Si vous souhaitez tester, appelez-moi !

PS: Nous allons devoir jouer avec votre fichier `hosts` si vous n'avez pas de nom de domaine. Mais vous allez voir c'est marrant.
:::

## La persistance des donnÃ©es

Nous avons vu comment dÃ©ployer une application, mais nous n'avons pas encore vu comment gÃ©rer la persistance des donnÃ©es. Dans le cas d'une application Â« classique Â» nous avons vu que nous pouvions utiliser un volume Docker pour sauvegarder les donnÃ©es.

### CrÃ©er un pod MariaDB utilisant un volume (persistant)

Les Pods que nous avons crÃ©Ã©s prÃ©cÃ©demment ne sauvegardai pas de donnÃ©es lors de leur exÃ©cution, si vous souhaitez sauvegarder des donnÃ©es et les rendre persistantes il faut crÃ©er un Volume (comme avec Docker). L'approche est relativement similaire, maisâ€¦ avec beaucoup de YAMLâ€¦ Ã‰normÃ©ment de YAML.

Mais si vous voulez une base de travail, voilÃ  un exemple de serveur MySQL utilisant un volume persistant pour sauvegarder les donnÃ©es de la base de donnÃ©es.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: fa-mysql
spec:
  ports:
    - port: 3306
  selector:
    app: fa-mysql
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: fa-mysql-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: fa-mysql-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fa-mysql
spec:
  selector:
    matchLabels:
      app: fa-mysql
  template:
    metadata:
      labels:
        app: fa-mysql
    spec:
      containers:
        - image: mariadb
          name: fa-mysql
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: "VOTRE_MOT_DE_PASSE"
          ports:
            - containerPort: 3306
              name: fa-mysql
          resources:
            limits:
              cpu: "1"
              memory: "512Mi"
            requests:
              cpu: "500m"
              memory: "256Mi"
          volumeMounts:
            - name: fa-mysql-persistent-storage
              mountPath: /var/lib/mysql
              subPath: mysql
      volumes:
        - name: fa-mysql-persistent-storage
          persistentVolumeClaim:
            claimName: fa-mysql-pv-claim
```

## GÃ©rer les accÃ¨s Ã  vos pods / services

Si vous avez crÃ©Ã© le serveur MySQL du point prÃ©cÃ©dent, vous souhaitez peut-Ãªtre maintenant y accÃ¨der pour faire par exempleâ€¦ des requÃªtes SQL ! Utiliser Kubernetes mÃªme en temps que dÃ©butant ne veux pas dire configurer n'importe comment votre serveur, si vous avez un service Â« non public Â», mais que vous souhaitez quand mÃªme y accÃ©der dans le cadre du test ou de la maintenance vous pouvez utiliser :

```sh
kubectl port-forward mariadb-75f59d57f4-4nd6q 3306:3306
```

Cette commande aura pour but de rendre accessible le port `3306` de la machine distante sur votre machine **et seulement sur votre machine**.

## Les certificats SSL

Nos applications sont maintenant dÃ©ployÃ©es, mais nous avons un problÃ¨meâ€¦ Elles ne sont pas sÃ©curisÃ©es. En effet, nous avons dÃ©ployÃ© nos applications sur des ports Â« classiques Â» (80). Cependant, nous n'avons pas configurÃ© de certificat SSL pour nos applications. Avec Kubernetes, il est possible de gÃ©rer les certificats SSL de maniÃ¨re automatique.

::: danger Attention
Cette configuration est plus complexe et ne fonctionne pas sur tous les clusters Kubernetes.

En effet, il faut que votre cluster soit accessible depuis l'extÃ©rieur pour que la configuration fonctionne.
:::

### Certificat SSL avec Let's Encrypt

Pour gÃ©rer les certificats SSL, nous allons utiliser [cert-manager](https://cert-manager.io/docs/). Cert-manager est un outil qui va nous permettre de gÃ©rer les certificats SSL de maniÃ¨re automatique. Pour l'installer, il suffit de suivre la documentation officielle :

```sh
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.3/cert-manager.yaml
```

VÃ©rifier que l'installation s'est bien dÃ©roulÃ©e :

```sh
kubectl get pods --namespace cert-manager
```

Vous devriez avoir un rÃ©sultat similaire Ã  :

```sh
NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-6f5b7c6d9f-4q9q8              1/1     Running   0          2m
cert-manager-cainjector-5f4f9f6d9f-4q9q8   1/1     Running   0          2m
cert-manager-webhook-7f5b7c6d9f-4q9q8      1/1     Running   0          2m
```

Une fois installÃ©s, nous allons pouvoir configurer notre cluster pour qu'il puisse demander des certificats SSL Ã  Let's Encrypt. Pour cela, nous allons devoir crÃ©er un `Issuer` et un `Certificate`.

```yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
  namespace: cert-manager
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: <your-email-address>
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress: { }
```

```sh
kubectl apply -f clusterissuer.yaml
```

VoilÃ ! Votre cluster est maintenant capable de demander des certificats SSL Ã  Let's Encrypt. Il ne reste plus qu'Ã  configurer votre application pour qu'elle utilise le certificat SSL.

PremiÃ¨re Ã©tape, il faut modifier notre fichier `services.yaml` pour Ã©couter sur le port `443` en plus du port `80`.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: vuepress-test
spec:
  selector:
    app: vuepress-test
  ports:
    - port: 80
      name: http
      targetPort: 80
      protocol: TCP
    - port: 443
      name: https
      targetPort: 443
      protocol: TCP
```

::: danger Vous Ãªtes chez un hÃ©bergeur (OVH, Scaleway, etc) ?

Vous devez ajouter l'ingress NGINX, celui-ci ajoutera le NameSpace nginx ainsi que le LoadBalancer (et son IP) pour que cert-manager puisse faire son travail.

```sh
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
helm -n ingress-nginx install ingress-nginx ingress-nginx/ingress-nginx --create-namespace
```

:::

Pour Ã§a nous allons devoir modifier notre fichier `ingress.yaml` Exemple, si votre nom de domaine est `press.domain.tld` et que vous souhaitez utiliser un certificat SSL pour votre application, il suffit de modifier le fichier `ingress.yaml` comme suit :

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: vuepress-test
  annotations:
    ingress.kubernetes.io/ssl-redirect: "false"
    acme.cert-manager.io/http01-edit-in-place: "true"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    kubernetes.io/ingress.class: nginx
spec:
  ingressClassName: nginx
  tls:
    - hosts:
      - press.domain.tld 
      secretName: letsencrypt-prod # Le nom du secret crÃ©Ã© par cert-manager (Dans notre cas letsencrypt-prod)
  rules:
    - host: press.domain.tld
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: vuepress-test
                port:
                  number: 80
```

Puis appliquer la configuration :

```sh
kubectl apply -f ingress.yaml
```

Lors de l'application de la configuration, cert-manager va automatiquement demander un certificat SSL Ã  Let's Encrypt.